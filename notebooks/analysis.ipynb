{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Experiment Analysis\n",
    "\n",
    "This notebook provides interactive analysis of the prompt engineering experiment results.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Data Loading\n",
    "2. Overall Performance Comparison\n",
    "3. Performance by Question Type\n",
    "4. Statistical Analysis\n",
    "5. Improvement vs Baseline\n",
    "6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "\n",
    "# Load statistics\n",
    "with open(results_dir / \"statistics.json\") as f:\n",
    "    statistics_data = json.load(f)\n",
    "\n",
    "# Load evaluations\n",
    "with open(results_dir / \"evaluations.json\") as f:\n",
    "    evaluations_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrames for easier analysis\n",
    "stats_df = pd.DataFrame(statistics_data).T\n",
    "stats_df.index.name = \"strategy\"\n",
    "stats_df = stats_df.reset_index()\n",
    "\n",
    "print(f\"Loaded statistics for {len(stats_df)} strategies\")\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation DataFrame\n",
    "eval_records = []\n",
    "for strategy, evals in evaluations_data.items():\n",
    "    for e in evals:\n",
    "        e[\"strategy\"] = strategy\n",
    "        eval_records.append(e)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_records)\n",
    "print(f\"Total evaluations: {len(eval_df)}\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing semantic similarity across strategies\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "strategies = stats_df[\"strategy\"]\n",
    "similarities = stats_df[\"mean_semantic_similarity\"]\n",
    "errors = stats_df[\"std_semantic_similarity\"]\n",
    "\n",
    "colors = sns.color_palette(\"husl\", len(strategies))\n",
    "bars = ax.bar(strategies, similarities, yerr=errors, capsize=5, color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Mean Semantic Similarity\")\n",
    "ax.set_title(\"Semantic Similarity Comparison Across Strategies\")\n",
    "\n",
    "for bar, val in zip(bars, similarities):\n",
    "    ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine distance comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "distances = stats_df[\"mean_cosine_distance\"]\n",
    "distance_errors = stats_df[\"std_cosine_distance\"]\n",
    "\n",
    "bars = ax.bar(strategies, distances, yerr=distance_errors, capsize=5, \n",
    "              color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Mean Cosine Distance\")\n",
    "ax.set_title(\"Cosine Distance Comparison (Lower is Better)\")\n",
    "\n",
    "for bar, val in zip(bars, distances):\n",
    "    ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match rate comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "match_rates = stats_df[\"exact_match_rate\"] * 100  # Convert to percentage\n",
    "\n",
    "bars = ax.bar(strategies, match_rates, color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Exact Match Rate (%)\")\n",
    "ax.set_title(\"Exact Match Rate Comparison\")\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "for bar, val in zip(bars, match_rates):\n",
    "    ax.annotate(f\"{val:.1f}%\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to get question types\n",
    "with open(Path.cwd().parent / \"data\" / \"raw\" / \"dataset.json\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "question_types = {q[\"id\"]: q[\"question_type\"] for q in dataset}\n",
    "eval_df[\"question_type\"] = eval_df[\"question_id\"].map(question_types)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by question type\n",
    "type_stats = eval_df.groupby([\"strategy\", \"question_type\"]).agg({\n",
    "    \"semantic_similarity\": [\"mean\", \"std\"],\n",
    "    \"cosine_distance\": [\"mean\", \"std\"],\n",
    "    \"exact_match\": \"mean\"\n",
    "}).round(3)\n",
    "\n",
    "type_stats.columns = [\"_\".join(col).strip() for col in type_stats.columns.values]\n",
    "type_stats = type_stats.reset_index()\n",
    "type_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart by question type\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "question_types_list = eval_df[\"question_type\"].unique()\n",
    "strategies_list = eval_df[\"strategy\"].unique()\n",
    "\n",
    "x = np.arange(len(question_types_list))\n",
    "width = 0.2\n",
    "colors = sns.color_palette(\"husl\", len(strategies_list))\n",
    "\n",
    "for i, strategy in enumerate(strategies_list):\n",
    "    values = []\n",
    "    for qt in question_types_list:\n",
    "        subset = eval_df[(eval_df[\"strategy\"] == strategy) & (eval_df[\"question_type\"] == qt)]\n",
    "        values.append(subset[\"semantic_similarity\"].mean())\n",
    "    \n",
    "    offset = (i - len(strategies_list)/2 + 0.5) * width\n",
    "    ax.bar(x + offset, values, width, label=strategy, color=colors[i], edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Question Type\")\n",
    "ax.set_ylabel(\"Mean Semantic Similarity\")\n",
    "ax.set_title(\"Performance by Question Type and Strategy\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(question_types_list)\n",
    "ax.legend(title=\"Strategy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cosine distances\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for strategy in strategies_list:\n",
    "    subset = eval_df[eval_df[\"strategy\"] == strategy]\n",
    "    ax.hist(subset[\"cosine_distance\"], bins=15, alpha=0.6, label=strategy, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Cosine Distance\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Distribution of Cosine Distances by Strategy\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of semantic similarity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(data=eval_df, x=\"strategy\", y=\"semantic_similarity\", ax=ax, palette=\"husl\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Semantic Similarity\")\n",
    "ax.set_title(\"Semantic Similarity Distribution by Strategy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary = eval_df.groupby(\"strategy\").agg({\n",
    "    \"semantic_similarity\": [\"count\", \"mean\", \"std\", \"min\", \"max\"],\n",
    "    \"cosine_distance\": [\"mean\", \"std\"],\n",
    "    \"exact_match\": [\"sum\", \"mean\"]\n",
    "})\n",
    "\n",
    "summary.columns = [\"_\".join(col) for col in summary.columns]\n",
    "summary = summary.round(4)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improvement vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement percentages vs baseline\n",
    "baseline_similarity = stats_df[stats_df[\"strategy\"] == \"baseline\"][\"mean_semantic_similarity\"].values[0]\n",
    "\n",
    "improvements = []\n",
    "for _, row in stats_df.iterrows():\n",
    "    if row[\"strategy\"] != \"baseline\":\n",
    "        improvement = ((row[\"mean_semantic_similarity\"] - baseline_similarity) / baseline_similarity) * 100\n",
    "        improvements.append({\n",
    "            \"strategy\": row[\"strategy\"],\n",
    "            \"improvement_pct\": improvement\n",
    "        })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvements)\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = [\"green\" if x >= 0 else \"red\" for x in improvement_df[\"improvement_pct\"]]\n",
    "bars = ax.bar(improvement_df[\"strategy\"], improvement_df[\"improvement_pct\"], \n",
    "              color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Improvement vs Baseline (%)\")\n",
    "ax.set_title(\"Performance Improvement Compared to Baseline Strategy\")\n",
    "\n",
    "for bar, val in zip(bars, improvement_df[\"improvement_pct\"]):\n",
    "    va = \"bottom\" if val >= 0 else \"top\"\n",
    "    ax.annotate(f\"{val:+.1f}%\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=va, fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of performance metrics\n",
    "metrics_for_heatmap = stats_df.set_index(\"strategy\")[[\n",
    "    \"mean_semantic_similarity\", \n",
    "    \"mean_cosine_distance\",\n",
    "    \"exact_match_rate\"\n",
    "]].T\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "sns.heatmap(metrics_for_heatmap, annot=True, fmt=\".3f\", cmap=\"RdYlGn\", ax=ax)\n",
    "ax.set_title(\"Performance Metrics Heatmap\")\n",
    "ax.set_ylabel(\"Metric\")\n",
    "ax.set_xlabel(\"Strategy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank strategies by performance\n",
    "ranking = stats_df.copy()\n",
    "ranking[\"rank_similarity\"] = ranking[\"mean_semantic_similarity\"].rank(ascending=False)\n",
    "ranking[\"rank_distance\"] = ranking[\"mean_cosine_distance\"].rank(ascending=True)\n",
    "ranking[\"rank_exact_match\"] = ranking[\"exact_match_rate\"].rank(ascending=False)\n",
    "ranking[\"overall_rank\"] = (ranking[\"rank_similarity\"] + ranking[\"rank_distance\"] + ranking[\"rank_exact_match\"]) / 3\n",
    "\n",
    "ranking = ranking.sort_values(\"overall_rank\")\n",
    "print(\"Strategy Ranking (Best to Worst):\")\n",
    "print(\"=\" * 50)\n",
    "for i, (_, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {row['strategy'].upper()}\")\n",
    "    print(f\"   Semantic Similarity: {row['mean_semantic_similarity']:.3f}\")\n",
    "    print(f\"   Cosine Distance:     {row['mean_cosine_distance']:.3f}\")\n",
    "    print(f\"   Exact Match Rate:    {row['exact_match_rate']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal questions evaluated: {len(dataset)}\")\n",
    "print(f\"Strategies compared: {len(stats_df)}\")\n",
    "print(f\"\\nBest performing strategy: {ranking.iloc[0]['strategy'].upper()}\")\n",
    "print(f\"  - Mean Semantic Similarity: {ranking.iloc[0]['mean_semantic_similarity']:.3f}\")\n",
    "print(f\"  - Mean Cosine Distance: {ranking.iloc[0]['mean_cosine_distance']:.3f}\")\n",
    "print(f\"  - Exact Match Rate: {ranking.iloc[0]['exact_match_rate']:.1%}\")\n",
    "\n",
    "if len(improvement_df) > 0:\n",
    "    best_improvement = improvement_df.loc[improvement_df[\"improvement_pct\"].idxmax()]\n",
    "    print(f\"\\nBest improvement over baseline: {best_improvement['strategy'].upper()}\")\n",
    "    print(f\"  - Improvement: {best_improvement['improvement_pct']:+.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
