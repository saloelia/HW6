{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Experiment Analysis\n",
    "\n",
    "This notebook provides interactive analysis of the prompt engineering experiment results.\n",
    "\n",
    "## Contents\n",
    "1. Setup and Data Loading\n",
    "2. Overall Performance Comparison\n",
    "3. Performance by Question Type\n",
    "4. Statistical Analysis\n",
    "5. Improvement vs Baseline\n",
    "6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 8)\n",
    "plt.rcParams[\"font.size\"] = 12\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load experiment results\n",
    "results_dir = Path.cwd().parent / \"results\"\n",
    "\n",
    "# Load statistics\n",
    "with open(results_dir / \"statistics.json\") as f:\n",
    "    statistics_data = json.load(f)\n",
    "\n",
    "# Load evaluations\n",
    "with open(results_dir / \"evaluations.json\") as f:\n",
    "    evaluations_data = json.load(f)\n",
    "\n",
    "# Convert to DataFrames for easier analysis\n",
    "stats_df = pd.DataFrame(statistics_data).T\n",
    "stats_df.index.name = \"strategy\"\n",
    "stats_df = stats_df.reset_index()\n",
    "\n",
    "print(f\"Loaded statistics for {len(stats_df)} strategies\")\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create evaluation DataFrame\n",
    "eval_records = []\n",
    "for strategy, evals in evaluations_data.items():\n",
    "    for e in evals:\n",
    "        e[\"strategy\"] = strategy\n",
    "        eval_records.append(e)\n",
    "\n",
    "eval_df = pd.DataFrame(eval_records)\n",
    "print(f\"Total evaluations: {len(eval_df)}\")\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar chart comparing semantic similarity across strategies\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "strategies = stats_df[\"strategy\"]\n",
    "similarities = stats_df[\"mean_semantic_similarity\"]\n",
    "errors = stats_df[\"std_semantic_similarity\"]\n",
    "\n",
    "colors = sns.color_palette(\"husl\", len(strategies))\n",
    "bars = ax.bar(strategies, similarities, yerr=errors, capsize=5, color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Mean Semantic Similarity\")\n",
    "ax.set_title(\"Semantic Similarity Comparison Across Strategies\")\n",
    "\n",
    "for bar, val in zip(bars, similarities):\n",
    "    ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine distance comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "distances = stats_df[\"mean_cosine_distance\"]\n",
    "distance_errors = stats_df[\"std_cosine_distance\"]\n",
    "\n",
    "bars = ax.bar(strategies, distances, yerr=distance_errors, capsize=5, \n",
    "              color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Mean Cosine Distance\")\n",
    "ax.set_title(\"Cosine Distance Comparison (Lower is Better)\")\n",
    "\n",
    "for bar, val in zip(bars, distances):\n",
    "    ax.annotate(f\"{val:.3f}\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exact match rate comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "match_rates = stats_df[\"exact_match_rate\"] * 100  # Convert to percentage\n",
    "\n",
    "bars = ax.bar(strategies, match_rates, color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Exact Match Rate (%)\")\n",
    "ax.set_title(\"Exact Match Rate Comparison\")\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "for bar, val in zip(bars, match_rates):\n",
    "    ax.annotate(f\"{val:.1f}%\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset to get question types\n",
    "with open(Path.cwd().parent / \"data\" / \"raw\" / \"dataset.json\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "question_types = {q[\"id\"]: q[\"question_type\"] for q in dataset}\n",
    "eval_df[\"question_type\"] = eval_df[\"question_id\"].map(question_types)\n",
    "\n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance by question type\n",
    "type_stats = eval_df.groupby([\"strategy\", \"question_type\"]).agg({\n",
    "    \"semantic_similarity\": [\"mean\", \"std\"],\n",
    "    \"cosine_distance\": [\"mean\", \"std\"],\n",
    "    \"exact_match\": \"mean\"\n",
    "}).round(3)\n",
    "\n",
    "type_stats.columns = [\"_\".join(col).strip() for col in type_stats.columns.values]\n",
    "type_stats = type_stats.reset_index()\n",
    "type_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart by question type\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "question_types_list = eval_df[\"question_type\"].unique()\n",
    "strategies_list = eval_df[\"strategy\"].unique()\n",
    "\n",
    "x = np.arange(len(question_types_list))\n",
    "width = 0.2\n",
    "colors = sns.color_palette(\"husl\", len(strategies_list))\n",
    "\n",
    "for i, strategy in enumerate(strategies_list):\n",
    "    values = []\n",
    "    for qt in question_types_list:\n",
    "        subset = eval_df[(eval_df[\"strategy\"] == strategy) & (eval_df[\"question_type\"] == qt)]\n",
    "        values.append(subset[\"semantic_similarity\"].mean())\n",
    "    \n",
    "    offset = (i - len(strategies_list)/2 + 0.5) * width\n",
    "    ax.bar(x + offset, values, width, label=strategy, color=colors[i], edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Question Type\")\n",
    "ax.set_ylabel(\"Mean Semantic Similarity\")\n",
    "ax.set_title(\"Performance by Question Type and Strategy\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(question_types_list)\n",
    "ax.legend(title=\"Strategy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of cosine distances\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for strategy in strategies_list:\n",
    "    subset = eval_df[eval_df[\"strategy\"] == strategy]\n",
    "    ax.hist(subset[\"cosine_distance\"], bins=15, alpha=0.6, label=strategy, edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"Cosine Distance\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Distribution of Cosine Distances by Strategy\")\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plot of semantic similarity\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "sns.boxplot(data=eval_df, x=\"strategy\", y=\"semantic_similarity\", ax=ax, palette=\"husl\")\n",
    "\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Semantic Similarity\")\n",
    "ax.set_title(\"Semantic Similarity Distribution by Strategy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics table\n",
    "summary = eval_df.groupby(\"strategy\").agg({\n",
    "    \"semantic_similarity\": [\"count\", \"mean\", \"std\", \"min\", \"max\"],\n",
    "    \"cosine_distance\": [\"mean\", \"std\"],\n",
    "    \"exact_match\": [\"sum\", \"mean\"]\n",
    "})\n",
    "\n",
    "summary.columns = [\"_\".join(col) for col in summary.columns]\n",
    "summary = summary.round(4)\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Improvement vs Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate improvement percentages vs baseline\n",
    "baseline_similarity = stats_df[stats_df[\"strategy\"] == \"baseline\"][\"mean_semantic_similarity\"].values[0]\n",
    "\n",
    "improvements = []\n",
    "for _, row in stats_df.iterrows():\n",
    "    if row[\"strategy\"] != \"baseline\":\n",
    "        improvement = ((row[\"mean_semantic_similarity\"] - baseline_similarity) / baseline_similarity) * 100\n",
    "        improvements.append({\n",
    "            \"strategy\": row[\"strategy\"],\n",
    "            \"improvement_pct\": improvement\n",
    "        })\n",
    "\n",
    "improvement_df = pd.DataFrame(improvements)\n",
    "improvement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = [\"green\" if x >= 0 else \"red\" for x in improvement_df[\"improvement_pct\"]]\n",
    "bars = ax.bar(improvement_df[\"strategy\"], improvement_df[\"improvement_pct\"], \n",
    "              color=colors, edgecolor=\"black\")\n",
    "\n",
    "ax.axhline(y=0, color=\"black\", linestyle=\"-\", linewidth=0.5)\n",
    "ax.set_xlabel(\"Prompting Strategy\")\n",
    "ax.set_ylabel(\"Improvement vs Baseline (%)\")\n",
    "ax.set_title(\"Performance Improvement Compared to Baseline Strategy\")\n",
    "\n",
    "for bar, val in zip(bars, improvement_df[\"improvement_pct\"]):\n",
    "    va = \"bottom\" if val >= 0 else \"top\"\n",
    "    ax.annotate(f\"{val:+.1f}%\", xy=(bar.get_x() + bar.get_width()/2, bar.get_height()),\n",
    "                ha=\"center\", va=va, fontsize=10, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Heatmap of performance metrics\nmetrics_for_heatmap = stats_df.set_index(\"strategy\")[[\n    \"mean_semantic_similarity\", \n    \"mean_cosine_distance\",\n    \"exact_match_rate\"\n]].T\n\n# Convert to numeric dtype\nmetrics_for_heatmap = metrics_for_heatmap.astype(float)\n\nfig, ax = plt.subplots(figsize=(10, 4))\n\nsns.heatmap(metrics_for_heatmap, annot=True, fmt=\".3f\", cmap=\"RdYlGn\", ax=ax)\nax.set_title(\"Performance Metrics Heatmap\")\nax.set_ylabel(\"Metric\")\nax.set_xlabel(\"Strategy\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Conclusions\n\n### Key Findings from the Experiment\n\nBased on the experiment results, here are the key observations:\n\n**1. Few-Shot Learning is the Clear Winner**\n- Achieved the highest semantic similarity (0.715) - a **+67.4% improvement** over baseline\n- Lowest cosine distance (0.285) indicating responses closest to expected answers\n- Maintained the same exact match rate (90%) as baseline\n- Fastest average execution time (596ms)\n\n**2. Chain of Thought and ReAct Performed Worse Than Baseline**\n- Both strategies showed ~47% and ~45% **degradation** respectively in semantic similarity\n- Higher cosine distances (~0.77) indicate responses diverged from expected answers\n- Lower exact match rates (86.7% for CoT, 83.3% for ReAct)\n- Significantly slower execution times (1572ms for CoT, 2561ms for ReAct)\n\n**3. Performance Varies by Question Type**\n- **Sentiment Analysis**: Few-shot achieved near-perfect similarity (~1.0), dramatically outperforming all others\n- **Math Problems**: Few-shot also excelled (~0.82 similarity) while CoT and ReAct struggled (~0.26)\n- **Logic Problems**: All strategies performed relatively poorly, with baseline actually performing best (0.39)\n\n**4. Why Did CoT and ReAct Underperform?**\n\nThe verbose nature of Chain of Thought and ReAct prompts likely caused:\n- **Answer format mismatch**: These strategies produce lengthy reasoning traces, while expected answers are concise\n- **Semantic drift**: The embedding similarity measures the entire response, so extra reasoning text dilutes the match\n- **Evaluation metric sensitivity**: The cosine distance metric penalizes responses that include reasoning alongside the answer"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank strategies by performance\n",
    "ranking = stats_df.copy()\n",
    "ranking[\"rank_similarity\"] = ranking[\"mean_semantic_similarity\"].rank(ascending=False)\n",
    "ranking[\"rank_distance\"] = ranking[\"mean_cosine_distance\"].rank(ascending=True)\n",
    "ranking[\"rank_exact_match\"] = ranking[\"exact_match_rate\"].rank(ascending=False)\n",
    "ranking[\"overall_rank\"] = (ranking[\"rank_similarity\"] + ranking[\"rank_distance\"] + ranking[\"rank_exact_match\"]) / 3\n",
    "\n",
    "ranking = ranking.sort_values(\"overall_rank\")\n",
    "print(\"Strategy Ranking (Best to Worst):\")\n",
    "print(\"=\" * 50)\n",
    "for i, (_, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {row['strategy'].upper()}\")\n",
    "    print(f\"   Semantic Similarity: {row['mean_semantic_similarity']:.3f}\")\n",
    "    print(f\"   Cosine Distance:     {row['mean_cosine_distance']:.3f}\")\n",
    "    print(f\"   Exact Match Rate:    {row['exact_match_rate']:.1%}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary with actual results\nprint(\"=\" * 70)\nprint(\"EXPERIMENT SUMMARY\")\nprint(\"=\" * 70)\n\nprint(\"\\nüìä EXPERIMENT CONFIGURATION\")\nprint(\"-\" * 70)\nprint(f\"Total questions evaluated: 30\")\nprint(f\"Question types: Sentiment Analysis, Math Problems, Logical Reasoning\")\nprint(f\"Strategies compared: 4 (Baseline, Few-Shot, Chain of Thought, ReAct)\")\n\nprint(\"\\nüèÜ STRATEGY RANKING (by Semantic Similarity)\")\nprint(\"-\" * 70)\nprint(\"1. FEW_SHOT        - Similarity: 0.715 | Distance: 0.285 | Match: 90.0%\")\nprint(\"2. BASELINE        - Similarity: 0.428 | Distance: 0.572 | Match: 90.0%\")\nprint(\"3. REACT           - Similarity: 0.234 | Distance: 0.766 | Match: 83.3%\")\nprint(\"4. CHAIN_OF_THOUGHT- Similarity: 0.227 | Distance: 0.773 | Match: 86.7%\")\n\nprint(\"\\nüìà IMPROVEMENT VS BASELINE\")\nprint(\"-\" * 70)\nprint(\"Few-Shot:          +67.4% improvement\")\nprint(\"ReAct:             -45.2% degradation\")\nprint(\"Chain of Thought:  -46.9% degradation\")\n\nprint(\"\\n‚è±Ô∏è EXECUTION TIME\")\nprint(\"-\" * 70)\nprint(\"Few-Shot:          596ms  (fastest)\")\nprint(\"Baseline:          938ms\")\nprint(\"Chain of Thought:  1572ms\")\nprint(\"ReAct:             2561ms (slowest)\")\n\nprint(\"\\nüí° KEY INSIGHT\")\nprint(\"-\" * 70)\nprint(\"Few-shot learning significantly outperforms all other strategies in this\")\nprint(\"experiment. The verbose reasoning in CoT and ReAct actually hurts semantic\")\nprint(\"similarity scores because the evaluation compares entire responses to\")\nprint(\"concise expected answers.\")\n\nprint(\"\\n\" + \"=\" * 70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}